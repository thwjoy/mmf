dataset_config:
  visual_genome:
      data_dir: ${env.data_dir}/datasets
      depth_first: false
      fast_read: false
      use_images: true
      use_features: false
      # features:
      #     train:
      #     - visual_genome/detectron_fix_100/fc6/,visual_genome/resnet152/
      #     val:
      #     - visual_genome/detectron_fix_100/fc6/,visual_genome/resnet152/
      #     test:
      #     - visual_genome/detectron_fix_100/fc6/,visual_genome/resnet152/
      images:
          train:
          - visual_genome/image/VG_100K/
          val:
          - visual_genome/image/VG_100K/
          test:
          - visual_genome/image/VG_100K/    
      annotations:
          train:
          - imdb/visual_genome/vg_question_answers.jsonl
          val:
          - imdb/visual_genome/vg_question_answers_placeholder.jsonl
          test:
          - imdb/visual_genome/vg_question_answers_placeholder.jsonl
      scene_graph_files:
          train:
          - imdb/visual_genome/vg_scene_graphs.jsonl
          val:
          - imdb/visual_genome/vg_scene_graphs_placeholder.jsonl
          test:
          - imdb/visual_genome/vg_scene_graphs_placeholder.jsonl
      max_features: 100
      processors:
        image_processor:
          type: torchvision_transforms
          params:
            transforms:
              - type: Resize
                params:
                  size: [256, 256]
              - type: CenterCrop
                params:
                  size: [224, 224]
              - ToTensor
              - GrayScaleTo3Channels
              - type: Normalize
                params:
                  mean: [0.46777044, 0.44531429, 0.40661017]
                  std: [0.12221994, 0.12145835, 0.14380469]
        text_processor:
          type: bert_tokenizer
          params:
            tokenizer_config:
              type: bert-base-uncased
              params:
                do_lower_case: true
            mask_probability: 0
            max_seq_length: 16
          # type: vocab
          # params:
          #   max_length: 14
          #   vocab:
          #     type: intersected
          #     embedding_name: glove.6B.300d
          #     vocab_file: vocabs/vocabulary_100k.txt
          #   preprocessor:
          #     type: simple_sentence
          #     params: {}
        answer_processor:
          type: vqa_answer
          params:
            num_answers: 1
            vocab_file: vocabs/answers_vqa.txt
            preprocessor:
              type: simple_word
              params: {}
        vg_answer_preprocessor:
          type: simple_word
          params: {}
        attribute_processor:
          type: vocab
          params:
            max_length: 2
            vocab:
              type: random
              vocab_file: vocabs/vocabulary_100k.txt
        name_processor:
          type: vocab
          params:
            max_length: 1
            vocab:
              type: random
              vocab_file: vocabs/vocabulary_100k.txt
        predicate_processor:
          type: vocab
          params:
            max_length: 2
            vocab:
              type: random
              vocab_file: vocabs/vocabulary_100k.txt
        synset_processor:
          type: vocab
          params:
            max_length: 1
            vocab:
              type: random
              vocab_file: vocabs/vg_synsets.txt
        bbox_processor:
          type: bbox
          params:
            max_length: 50
      return_scene_graph: true
      return_objects: true
      return_relationships: true
      return_features_info: true
      no_unk: false
      # Return OCR information
      use_ocr: false
      # Return spatial information of OCR tokens if present
      use_ocr_info: false
