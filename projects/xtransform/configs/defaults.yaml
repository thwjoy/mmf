training:
  max_updates: 100000
  checkpoint_interval: 10000
  evaluation_interval: 1000
  batch_size: 256 # 32 per GPU * 8 GPU
  log_interval: 1000
  # find_unused_parameters: True
  

optimizer:
  type: adam_w
  params:
    lr: 1e-4
    eps: 1e-8
    weight_decay: 1e-2
